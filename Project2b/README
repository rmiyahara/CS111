NAME: Ryan Miyahara
EMAIL: rmiyahara144@gmail.com
ID: 804585999
SLIPDAYS: 2

------------------
| Included Files |
------------------
Within this submission are the following files:
SortedList.h
Describes the Doubly Linked List interface. Provided by the class.

SortedList.c
Implements the Doubly Linked List interface. Provides the user with the following functions:
void SortedList_insert(SortedList_t *list, SortedListElement_t *element);
Inserts element into list while maintaining the sorted attribute of the list.
int SortedList_delete( SortedListElement_t *element);
Deletes an element from the linked list.
SortedListElement_t *SortedList_lookup(SortedList_t *list, const char *key);
Returns a pointer to the element containing key. If it doesn't exist, NULL is returned.
int SortedList_length(SortedList_t *list);
Returns the length of list.

lab2_list.c
This file compiles to make an executable which creates a certain number of threads
which inserts, deletes, or looks up a certain number of times. The executable is
specified in the following way:
$ ./lab2_add [--threads=#threads --iterations=#iterations --yield=[idl] --sync=[ms] --debug]
--threads=#threads
The threads flag allows the user to set the number of threads that the program creates. If
this flag isn't used, only 1 thread is specified.
--iterations=#iterations
The iterations flag allows the user to set the number of times the counter is added and
subtracted from. If this flag isn't used, 1 iteration is specified.
--yield=[idl]
This flag takes no arguments and calls the POSIX sched_yield() function with every addition.
i refers to insert, d refers to delete, and l refers to lookup. Each of these performs an
action on the list.
--sync=[ms]
This flag prevents synchronization errors from happening with the use of a mutex(m),
spin-lock(s), or a compare-and-swap(c).
--list=#sublists
This flag takes in an integer and creates the specified number of sublists. Each of these can
be specified by a hash of its key, has its own header, and synchronization object.
--debug
Gives the user helpful output for debugging.
The executable uses the following exit codes:
0: successful run
1: invalid argument or system call error
2: other failures ... e.g. list operation failures due to conflicting updates

Makefile
Builds all the executables and has the following targets:
build (default)
Compiles all programs with the -Wall, -Wextra, and -pthread flags.
tests
Runs all of the test cases for the lab2_add.c and lab2_list.c programs. The results are then
stored in lab2_add.csv and lab2_list.csv respectively.
graphs
Uses gnuplot(1), lab2_add.csv, and lab2_list.csv to create all of the attached graphs.
dist
Creates the deliverable tarball
clean
Deletes all programs and output created by the Makefile

lab2b_list.csv
Holds data from all of the tests done on lab2_list.c. The data from this file is used to make
lab2b_*.png

profile.out
This file is an execution profiling report showing where time was spent in the un-partitioned
spin-lock implementation.

lab2b_1.png
This is a graph that plots throughput vs. number of threads for mutex and spin-lock
synchronized list operations.

lab2_2.png
This is a graph that plots mean time per mutex wait and mean time per operation for mutex-
synchronized list operations.

lab2_3.png
This is a graph that plots successful iterations vs. threads for each synchronization method.

lab2_4.png
This is a graph that plots throughput vs. number of threads for mutex synchronized
partitioned lists.

lab2_5.png
This is a graph that plots throughput vs. number of threads for spin-lock-synchronized
partitioned lists.

plotlist.sh
This shell script performs all the tests on lab2_list.c to provide the data in
lab2_list.csv.

lab2_list.gp
This shell script creates lab2_*.png by graphing the data from lab2b_list.csv.

-------------
| Questions |
-------------
2.3.1 - CPU time in the basic list implementation
Most of the CPU time in the 1 and 2-thread list tests are spent performing the list methods.
This is due to to not having to constantly spin threads or wait on mutexes to be unlocked.
Most of the CPU time, in the high-thread-spin lock tests are being spent spinning waiting threads
because only 1 thread can operate on the list at a time. In the mutex case, since threads are slept
rather than spun, most of the CPU time is being used on list methods. It should be noted however,
that CPU time is still spent on unlocking the mutexes.

2.3.2 - Execution Profiling
The reoccurring line:
while(__sync_lock_test_and_set(&spin_lockerino, 1));
spins the threads who do not have access to the lock. This is where most of the CPU time goes in
the case of high-thread counts. This operation is very expensive in those cases because it forces
all the threads, besides 1, to keep spinning until another one has access to the lock.

2.3.3 - Mutex Wait Time
The average lock-wait time rises so dramatically with the number of contending threads because
only 1 thread can have access to the lock at a time. This means all other threads must wait on the
lock for a longer period of time. The completion time per operation rises with the number of contending
threads because of the increase in context switches. It is possible for the wait time per operation to
increase faster than the completion time per operation because lock-wait is counted by all threads at
the same time, while completion time is only recorded while the thread isn't spinning. This means lock-
wait time will always increase faster than time per operation.

2.3.4 - Performance of Partitioned Lists
The performance of the synchronized methods improves as the number of lists increases. This is because
there are less threads waiting on each lock. The throughput will increase up until lock-wait no longer
becomes the bottleneck. Once this is reached, we will no longer see an improvement in performance because
the new bottleneck will be list methods which will not be affected by the number of lists. The throughput
of a single list with fewer (1/N) threads is equivalent to the throughput of an N-way partitioned
list in this case because of each sublist's equivalent size. When this isn't the case, each sublist will
have a different bottleneck causing the throughput to be different.

--------------
| References |
--------------
Much of the code has been recycled from my Project2a:
https://github.com/rmiyahara/CS111/tree/master/Project2a

man pages:
http://man7.org/linux/man-pages/man2/clock_gettime.2.html
https://gcc.gnu.org/onlinedocs/gcc-4.4.3/gcc/Atomic-Builtins.html
https://linux.die.net/man/1/gnuplot

pthread tutorial
https://computing.llnl.gov/tutorials/pthreads/
