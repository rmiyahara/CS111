NAME: Ryan Miyahara
EMAIL: rmiyahara144@gmail.com
ID: 804585999
SLIPDAYS: 2

------------------
| Included Files |
------------------
Within this submission are the following files:
lab2_add.c
This file compiles to make an executable which creates a certain number of threads
which adds and subtracts from a shared counter a given number of times. The executable is
specified in the following way:
$ ./lab2_add [--threads=#threads --iterations=#iterations --yield --sync=[msc] --debug]
--threads=#threads
The threads flag allows the user to set the number of threads that the program creates. If
this flag isn't used, only 1 thread is specified.
--iterations=#iterations
The iterations flag allows the user to set the number of times the counter is added and
subtracted from. If this flag isn't used, 1 iteration is specified.
--yield
This flag takes no arguments and calls the POSIX sched_yield() function with every addition.
This slows down the program by a lot and is explained in the questions section.
--sync=[msc]
This flag prevents synchronization errors from happening with the use of a mutex(m),
spin-lock(s), or a compare-and-swap(c).
--debug
Gives the user helpful output for debugging.

lab2_add-1.png
This is a graph which plots the existance of successful runs vs the number of threads and
iterations.

lab2-add-2.png
This is a graph which plots the time per operation vs the number of iterations for yield and
non-yield executions.

lab2-add-3.png
This is a graph which plots, for a single thread, the average cost per operation (non-yield)
as a function of the number of iterations.

lab2-add-4.png
This is a graph which plots the (existence of) successful runs vs the number of threads and
iterations for each synchronization option (none, mutex, spin, compare-and-swap)

lab2-add-5.png
This is a graph which plots the average time per operation (non-yield), vs the number of threads.

plotadd.sh
This shell script runs all cases of the lab2_add.c program and records them into add2_add.csv.
The purpose of this is to create data for lab2_add.gp.

lab2_add.csv
Contains all the test data from the lab2_add.c program. It runs thread counts 1, 2, 4, 8, 12 and
iterations 10, 20, 40, 80, 100, 1000, 10000, 100000 for most cases. This is then utilized by
lab2_add.gp to graph lab2_add-*.png.

lab2_add.gp
Creates lab2_add-*.png from lab2_add.csv.

SortedList.h
Describes the Doubly Linked List interface. Provided by the class.

SortedList.c
Implements the Doubly Linked List interface. Provides the user with the following functions:
void SortedList_insert(SortedList_t *list, SortedListElement_t *element);
Inserts element into list while maintaining the sorted attribute of the list.
int SortedList_delete( SortedListElement_t *element);
Deletes an element from the linked list.
SortedListElement_t *SortedList_lookup(SortedList_t *list, const char *key);
Returns a pointer to the element containing key. If it doesn't exist, NULL is returned.
int SortedList_length(SortedList_t *list);
Returns the length of list.

lab2_list.c
This file compiles to make an executable which creates a certain number of threads
which inserts, deletes, or looks up a certain number of times. The executable is
specified in the following way:
$ ./lab2_add [--threads=#threads --iterations=#iterations --yield=[idl] --sync=[ms] --debug]
--threads=#threads
The threads flag allows the user to set the number of threads that the program creates. If
this flag isn't used, only 1 thread is specified.
--iterations=#iterations
The iterations flag allows the user to set the number of times the counter is added and
subtracted from. If this flag isn't used, 1 iteration is specified.
--yield=[idl]
This flag takes no arguments and calls the POSIX sched_yield() function with every addition.
i refers to insert, d refers to delete, and l refers to lookup. Each of these performs an
action on the list.
--sync=[ms]
This flag prevents synchronization errors from happening with the use of a mutex(m),
spin-lock(s), or a compare-and-swap(c).
--debug
Gives the user helpful output for debugging.

lab2_list-1.png
This is a graph which plots the mean cost per operation vs the number of iterations.

lab2_list-2.png
This is a graph which plots the successful runs vs the number of threads and iterations for
non-yield and each of the above four yield combinations.

lab2_list-3.png
This is a graph which plots the (existence of) successful runs vs the number of iterations for
mutex and spin-lock protection with each of the above four yield combinations.

lab2_list-4.png
This is a graph which plots the (corrected for list length) per operation times (for each of
the synchronization options: mutex, spin) vs the number of threads.

plotlist.sh
This shell script runs all cases of the lab2_list.c program and records them int lab2_list.csv.
The purpose of this is to create data for lab2_list.gp.

lab2_list.csv
Contains all the test data from the lab2_list.c program. It runs thread counts 1, 2, 4, 8, 12,
16, 64. The iterations vary by test. The data this script makes is used to make lab2_list-*.png.

lab2_list.gp
Creates lab2_list-*.png from lab2_list.csv.

Makefile
Builds all the executables and has the following targets:
build (default)
Compiles all programs with the -Wall, -Wextra, and -pthread flags.
tests
Runs all of the test cases for the lab2_add.c and lab2_list.c programs. The results are then
stored in lab2_add.csv and lab2_list.csv respectively.
graphs
Uses gnuplot(1), lab2_add.csv, and lab2_list.csv to create all of the attached graphs.
dist
Creates the deliverable tarball
clean
Delets all programs and output created by the Makefile

--------------
| References |
--------------
Some code taken from my Project1a
https://github.com/rmiyahara/CS111/tree/master/Project1a

man pages
http://man7.org/linux/man-pages/man2/clock_gettime.2.html
https://gcc.gnu.org/onlinedocs/gcc-4.4.3/gcc/Atomic-Builtins.html
https://linux.die.net/man/1/gnuplot

pthread tutorial
https://computing.llnl.gov/tutorials/pthreads/

-------------
| Questions |
-------------
2.1.1 - causing conflicts
It takes many iterations before errors are seen because non-deterministic errors are unlikely to
happen. Since making a thread takes longer than modifying the shared counter, interrupts are less
likely to happen during the critical section of code. A significantly smaller number of
iterations seldom fails because the shared variable is accessed much less often which means there
are less chances for an error. 

2.1.2 - cost of yielding
The --yield runs are much slower because a system call is made. This raises the overhead as
every time a system is called, the OS much switch from user mode to kernel mode. We cannot receive
a valid per-operation timings if the yield option is used because we would now be including the
system call rather than solely the operation.

2.1.3 - measurement errors
The average cost per operation drops with an increase of iterations because thread creation overhead
is fixed. This means that an increase in iterations results in the creation overhead being more
spread out. If the cost per iteration is a function of the number of iterations, we know how many
iterations to run by seeing when the slope becomes 1.

2.1.4 - costs of serialization
All of the options perform similarly for low numbers of threads because there are less threads to
spin in the lock. This means each thread waits less for the lock. All three protected operations
slow down as the number of threads rises because each thread now has to wait longer for the critical
section to become available.

2.2.1 - scalablility of mutex
The time per mutex-protected operation vs the number of threads in Part-1 and Part-2 differ in shape
because Part-2's slope is much steeper. This happens because mutex operations in Part-2 are much more
intensive than Part-1's simple adder. Thus, the threads have to wait much longer to obtain critical
points in the code.

2.2.2 - scalability of spin locks
According to the graphs, spin locks increase time per operation at a faster rate than mutexes with
increasing threads. This can be seen on a spin locks much steeper slope. This happens because spin
locks waste CPU cycles by spinning threads while mutexes put threads to sleep instead.